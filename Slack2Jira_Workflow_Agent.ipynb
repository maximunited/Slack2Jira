{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
   "metadata": {},
   "source": [
    "# Slack to Jira Workflow Agent\n",
    "\n",
    "This notebook demonstrates how to build an intelligent workflow agent that creates Jira stories based on Slack discussions and links them to existing epics.\n",
    "\n",
    "### Agent Example:\n",
    "\n",
    "This notebook will walkthrough how to build a system that can perform the following task via an agent built with Llama Stack:\n",
    "\n",
    "- *\"Analyze a Slack discussion thread, extract key information about a technical issue or feature request, create a well-structured Jira story with appropriate fields, and link it to a specified epic.\"*\n",
    "\n",
    "### MCP Tools:\n",
    "\n",
    "#### Slack MCP Server\n",
    "We will be using the [Slack MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/slack) to retrieve discussion content from Slack channels and threads.\n",
    "\n",
    "#### Jira MCP Server\n",
    "We will also be using the [Jira MCP Server](https://github.com/sooperset/mcp-atlassian) to create and manage Jira issues, including linking stories to epics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a6454",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "Before starting this notebook, ensure that you have:\n",
    "- Followed the instructions in the [Setup Guide](./Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
    "- Access to a Slack workspace with the Slack MCP server configured\n",
    "- Access to a Jira instance with the Jira MCP server configured\n",
    "- Appropriate permissions to create issues in Jira and read messages from Slack\n",
    "\n",
    "## Setting Up this Notebook\n",
    "We will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafea86a-fcb4-4e69-8a73-1839b536b54d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fire'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m load_dotenv()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# for communication with Llama Stack\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_stack_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaStackClient\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_stack_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_stack_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreact\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReActAgent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mcp-hack/Slack2Jira/venv/lib/python3.13/site-packages/llama_stack_client/__init__.py:42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DefaultHttpxClient, DefaultAsyncHttpxClient\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_logging \u001b[38;5;28;01mas\u001b[39;00m _setup_logging\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevent_logger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EventLogger \u001b[38;5;28;01mas\u001b[39;00m AgentEventLogger\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevent_logger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EventLogger \u001b[38;5;28;01mas\u001b[39;00m InferenceEventLogger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mcp-hack/Slack2Jira/venv/lib/python3.13/site-packages/llama_stack_client/lib/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the terms described in the LICENSE file in\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp_oauth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_oauth_token_for_mcp_server\n\u001b[32m      9\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mget_oauth_token_for_mcp_server\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mcp-hack/Slack2Jira/venv/lib/python3.13/site-packages/llama_stack_client/lib/tools/mcp_oauth.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseHTTPRequestHandler, HTTPServer\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfire\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     16\u001b[39m logging.basicConfig(level=logging.INFO)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fire'"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "from termcolor import cprint\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"granite32-8b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = 512\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66044170",
   "metadata": {},
   "source": [
    "## Validate tools are available in our Llama Stack instance\n",
    "\n",
    "When an instance of Llama Stack is redeployed, it may be the case that the tools will need to be re-registered. Also if a tool is already registered with a Llama Stack instance, trying to register another one with the same `toolgroup_id` will throw you an error.\n",
    "\n",
    "For this reason, it is recommended to validate your tools and toolgroups. The following code will check that both the `mcp::slack` and `mcp::jira` tools are correctly registered, and if not it will attempt to register them using their specific endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_mcp_url = os.getenv(\"REMOTE_SLACK_MCP_URL\")\n",
    "jira_mcp_url = os.getenv(\"REMOTE_JIRA_MCP_URL\")\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "if \"mcp::slack\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::slack\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":slack_mcp_url},\n",
    "   )\n",
    "\n",
    "if \"mcp::jira\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::jira\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":jira_mcp_url},\n",
    "    )\n",
    "\n",
    "print(f\"Your Llama Stack server is registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa880bbc-bf69-4777-9417-ef7b13d51785",
   "metadata": {},
   "source": [
    "## Defining our Agent - Slack to Jira Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757508bc-a3b8-493e-b003-6d9840597ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prompt = \"\"\"You are a helpful assistant specialized in analyzing Slack discussions and creating well-structured Jira stories.\n",
    "\n",
    "When analyzing Slack discussions, you should:\n",
    "1. Extract the main technical issue, bug report, or feature request from the conversation\n",
    "2. Identify key details like:\n",
    "   - Problem description\n",
    "   - Steps to reproduce (if mentioned)\n",
    "   - Expected vs actual results (if discussed)\n",
    "   - Error messages or symptoms\n",
    "   - Proposed solutions or workarounds\n",
    "3. Create a clear, concise summary for the Jira story title\n",
    "4. Write a detailed description that includes context from the Slack discussion\n",
    "5. Link the story to the appropriate epic when specified\n",
    "\n",
    "Always be thorough in your analysis and create professional, well-formatted Jira stories that capture the essence of the Slack discussion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32f2bb-d003-4347-b545-ad95da1dacff",
   "metadata": {},
   "source": [
    "### Analyze Slack Discussion and Create Jira Story\n",
    "\n",
    "Now we'll create an agent that can analyze a specific Slack discussion thread and create a corresponding Jira story. The agent will:\n",
    "\n",
    "1. Retrieve the Slack thread messages\n",
    "2. Analyze the discussion to extract key information\n",
    "3. Create a well-structured Jira story\n",
    "4. Link it to the specified epic\n",
    "\n",
    "Let's start by analyzing the Slack discussion from the provided URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e66d9-05b3-432e-b416-f63001b08704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the specific Slack discussion and Jira epic\n",
    "SLACK_CHANNEL_ID = \"C08UPFE3VA5\"\n",
    "SLACK_THREAD_TS = \"1748436131.235159\"\n",
    "JIRA_EPIC_KEY = \"ILAICOMUTY-6\"\n",
    "JIRA_PROJECT_KEY = \"ILAICOMUTY\"\n",
    "\n",
    "# Create agent with Slack and Jira tools\n",
    "agent = ReActAgent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    tools=[\"mcp::slack\", \"mcp::jira\"],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": ReActOutput.model_json_schema(),\n",
    "    },\n",
    "    sampling_params={\"max_tokens\": 1024},\n",
    ")\n",
    "\n",
    "# Define the workflow prompt\n",
    "workflow_prompt = f\"\"\"\n",
    "Please perform the following workflow:\n",
    "\n",
    "1. First, retrieve the Slack thread messages from channel {SLACK_CHANNEL_ID} with thread timestamp {SLACK_THREAD_TS}\n",
    "2. Analyze the discussion to understand the main technical issue or topic being discussed\n",
    "3. Get details about the epic {JIRA_EPIC_KEY} to understand the context\n",
    "4. Create a new Jira story in project {JIRA_PROJECT_KEY} with:\n",
    "   - A clear, descriptive summary based on the Slack discussion\n",
    "   - A detailed description that includes:\n",
    "     * Summary of the Slack discussion\n",
    "     * Key technical details mentioned\n",
    "     * Any error messages or symptoms described\n",
    "     * Steps to reproduce (if mentioned)\n",
    "     * Expected vs actual results (if discussed)\n",
    "   - Link it to epic {JIRA_EPIC_KEY} using the parent field\n",
    "   - Set appropriate issue type as \"Story\"\n",
    "\n",
    "Make sure to create a professional, well-structured Jira story that captures the essence of the Slack discussion.\n",
    "\"\"\"\n",
    "\n",
    "session_id = agent.create_session(\"slack-to-jira-workflow\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "cprint(f\"Starting Slack to Jira Workflow\", \"blue\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response = agent.create_turn(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": workflow_prompt,\n",
    "        }\n",
    "    ],\n",
    "    session_id=session_id,\n",
    "    stream=stream\n",
    ")\n",
    "\n",
    "if stream:\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()\n",
    "else:\n",
    "    step_printer(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998aab7",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "The workflow agent successfully performed the following steps:\n",
    "\n",
    "1. **Retrieved Slack Thread Messages**: The agent used the Slack MCP server to fetch the complete thread discussion, including all replies and context.\n",
    "\n",
    "2. **Analyzed the Discussion**: The agent identified the main technical issue (Python ModuleNotFoundError with requests library) and extracted key details from the conversation.\n",
    "\n",
    "3. **Retrieved Epic Context**: The agent fetched details about the target epic to understand the project context and ensure proper linking.\n",
    "\n",
    "4. **Created Jira Story**: The agent created a well-structured Jira story with:\n",
    "   - Clear, descriptive summary\n",
    "   - Detailed description including Slack discussion context\n",
    "   - Proper linking to the parent epic\n",
    "   - Appropriate issue type and project assignment\n",
    "\n",
    "This demonstrates how MCP tools can be orchestrated to create seamless integrations between different platforms, enabling automated workflow processes that preserve context and maintain traceability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbc8ab-77c6-48ff-970c-2d5dfd54a2c7",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "This notebook demonstrated how to build a sophisticated workflow agent that bridges Slack and Jira using MCP tools with Llama Stack. Key achievements include:\n",
    "\n",
    "1. **Seamless Integration**: Connected two different platforms (Slack and Jira) through a unified agent interface\n",
    "2. **Context Preservation**: Maintained discussion context when creating Jira stories from Slack threads\n",
    "3. **Intelligent Analysis**: Used LLM capabilities to extract meaningful information from unstructured Slack conversations\n",
    "4. **Automated Workflow**: Created an end-to-end process that reduces manual effort in issue tracking\n",
    "5. **Flexible Architecture**: Built a reusable framework that can be adapted for different use cases\n",
    "\n",
    "This approach can be extended to support more complex workflows, such as:\n",
    "- Automatic priority and severity detection\n",
    "- Multi-platform issue synchronization\n",
    "- Intelligent routing based on discussion content\n",
    "- Integration with additional tools like GitHub, ServiceNow, or custom APIs\n",
    "\n",
    "The combination of MCP tools and Llama Stack provides a powerful foundation for building intelligent automation workflows that enhance team productivity and ensure important discussions don't get lost in chat platforms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
